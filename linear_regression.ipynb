{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import logging\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "from wavlm.WavLM import WavLM, WavLMConfig\n",
    "from hifigan.models import Generator as HiFiGAN\n",
    "from hifigan.utils import AttrDict\n",
    "from matcher import ExNOTVC\n",
    "import torchaudio\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm\n",
    "\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xnot_vc(pretrained=True, progress=True, prematched=True, device='cuda') -> ExNOTVC:\n",
    "    \"\"\" Load kNN-VC (WavLM encoder and HiFiGAN decoder). Optionally use vocoder trained on `prematched` data. \"\"\"\n",
    "    hifigan, hifigan_cfg = hifigan_wavlm(pretrained, progress, prematched, device)\n",
    "    wavlm = wavlm_large(pretrained, progress, device)\n",
    "    xnotvc = ExNOTVC(wavlm, hifigan, hifigan_cfg, device)\n",
    "    return xnotvc\n",
    "\n",
    "\n",
    "def hifigan_wavlm(pretrained=True, progress=True, prematched=True, device='cuda') -> HiFiGAN:\n",
    "    \"\"\" Load pretrained hifigan trained to vocode wavlm features. Optionally use weights trained on `prematched` data. \"\"\"\n",
    "    #cp = Path(__file__).parent.absolute()\n",
    "\n",
    "    with open('hifigan/config_v1_wavlm.json') as f:\n",
    "        data = f.read()\n",
    "    json_config = json.loads(data)\n",
    "    h = AttrDict(json_config)\n",
    "    device = torch.device(device)\n",
    "\n",
    "    generator = HiFiGAN(h).to(device)\n",
    "    \n",
    "    if pretrained:\n",
    "        if prematched:\n",
    "            url = \"https://github.com/bshall/knn-vc/releases/download/v0.1/prematch_g_02500000.pt\"\n",
    "        else:\n",
    "            print(\"Загружаем непреметченный\")\n",
    "            url = \"https://github.com/bshall/knn-vc/releases/download/v0.1/g_02500000.pt\"\n",
    "        state_dict_g = torch.hub.load_state_dict_from_url(\n",
    "            url,\n",
    "            map_location=device,\n",
    "            progress=progress\n",
    "        )\n",
    "        generator.load_state_dict(state_dict_g['generator'])\n",
    "    generator.eval()\n",
    "    generator.remove_weight_norm()\n",
    "    print(f\"[HiFiGAN] Generator loaded with {sum([p.numel() for p in generator.parameters()]):,d} parameters.\")\n",
    "    return generator, h\n",
    "\n",
    "\n",
    "def wavlm_large(pretrained=True, progress=True, device='cuda') -> WavLM:\n",
    "    \"\"\"Load the WavLM large checkpoint from the original paper. See https://github.com/microsoft/unilm/tree/master/wavlm for details. \"\"\"\n",
    "    if torch.cuda.is_available() == False:\n",
    "        if str(device) != 'cpu':\n",
    "            logging.warning(f\"Overriding device {device} to cpu since no GPU is available.\")\n",
    "            device = 'cpu'\n",
    "    checkpoint = torch.hub.load_state_dict_from_url(\n",
    "        \"https://github.com/bshall/knn-vc/releases/download/v0.1/WavLM-Large.pt\", \n",
    "        map_location=device, \n",
    "        progress=progress\n",
    "    )\n",
    "    \n",
    "    cfg = WavLMConfig(checkpoint['cfg'])\n",
    "    device = torch.device(device)\n",
    "    model = WavLM(cfg)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    print(f\"WavLM-Large loaded with {sum([p.numel() for p in model.parameters()]):,d} parameters.\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wlad/Desktop/Projects/Bridge Matching/ml_env/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing weight norm...\n",
      "[HiFiGAN] Generator loaded with 16,523,393 parameters.\n",
      "WavLM-Large loaded with 315,453,120 parameters.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "xnotvc = xnot_vc() #загружаем веса для WavLM, HiFi Gan, убираем флажок преметчинга \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пути к аудиодорожкам с женским голосом и мужским"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Domain source - домен с исходным голосом, Domain target - домен с целевым голосом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_target_path = \"experiments/2300/to_male/male_8455.flac\"\n",
    "domain_source_path = \"experiments/2300/male_2300.flac\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кодируем аудиодорожки с помощью WavLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_source = xnotvc.get_features(domain_source_path).to(\"cpu\")\n",
    "features_target = xnotvc.get_features(domain_target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "target = xnotvc.match(features_source, features_target).to('cpu')\n",
    "# Split the data into training (80%) and test (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_source, target, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 1.0462\n"
     ]
    }
   ],
   "source": [
    "model = Ridge(alpha=0.1)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 1.0462\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_transform(source_tensor):\n",
    "    result = np.empty((source_tensor.shape[0],1024)) #Initialize empty tensor with the same dimensions as an input feature vector\n",
    "    for i in range(source_tensor.shape[0]):\n",
    "        result[i] = model.predict(source_tensor[i])\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_path = \"experiments/2300/valid_long.flac\" #Путь до файла с женским голосом для валидации\n",
    "features_valid = xnotvc.get_features(validation_path).to(\"cpu\") #Формируем массив из векторов фичей\n",
    "feautures_transformed = torch.Tensor(model.predict(features_valid)).to(\"cuda\")#Преобразованный массив\n",
    "#feautures_transformed = regression_transform(features_valid) #Преобразованный массив\n",
    "out_wav = xnotvc.vocode(feautures_transformed[None].to(DEVICE)).cpu().squeeze()\n",
    "torchaudio.save('experiments/2300/to_male/results/linreg/linreg_2300_8455.flac', out_wav[None], 16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **KNNVC TEST** (skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_path = \"experiments/2300/valid_long.flac\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_wav_paths = [domain_target_path] #target\n",
    "src_wav_path = validation_path  #validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_seq = xnotvc.get_features(src_wav_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_set = xnotvc.get_matching_set(ref_wav_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_wav = xnotvc.match(query_seq, matching_set, topk=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchaudio.save('experiments/2300/to_female/results/knnvc_2300_3570_30_long.flac', out_wav[None], 16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **END OF TEST**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WavLM каждые ~20-30 мс звуковой дорожки представляет в виде вектора фичей размерностью 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features_source.shape) #[длина дорожки делить на ~20-30мс , 1024] = массив из 7971 векторов размерностью 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features_target.shape) #массив из 5473 векторов размерностью 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampler from voice distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Случайным образом выбираем батч векторов фичей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_tensor(t, batch_size = 32):\n",
    "    tensor = t.detach().cpu().numpy()\n",
    "    batch = np.empty((batch_size, 1024))\n",
    "    for i in range(batch_size):\n",
    "        indice = random.randrange(0, tensor.shape[0])\n",
    "        sample_tensor = tensor[indice]\n",
    "        batch[i]=sample_tensor\n",
    "    return batch.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_from_tensor(features_source).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_from_tensor(features_target).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция стоимости"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sq_cost(X,Y):\n",
    "    return (X-Y).square().flatten(start_dim=1).mean(dim=1)\n",
    "\n",
    "COST = sq_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define NNs for transport map ***T*** : R^1024 -> R^1024 (generator) and potential ***f*** : R^1024 -> R (discriminator).\n",
    "Medium size multilayer perceptrons with ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegAbs(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NegAbs, self).__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return -torch.abs(input)\n",
    "\n",
    "T = nn.Sequential(\n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(1024, 1024),\n",
    ").to(DEVICE)\n",
    "\n",
    "f = nn.Sequential(\n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(1024, 1),\n",
    "    NegAbs(),\n",
    ").to(DEVICE)\n",
    "\n",
    "def weight_reset(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        m.reset_parameters()\n",
    "\n",
    "\n",
    "print('T params:', np.sum([np.prod(p.shape) for p in T.parameters()]))\n",
    "print('f params:', np.sum([np.prod(p.shape) for p in f.parameters()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оптимизаторы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "T_opt = torch.optim.Adam(T.parameters(), lr=1e-4, weight_decay=1e-10)\n",
    "f_opt = torch.optim.Adam(f.parameters(), lr=1e-4, weight_decay=1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "T_ITERS = 10 # T updates per 1 f update\n",
    "MAX_ITERS = 30001\n",
    "\n",
    "to_torch = lambda x: torch.Tensor(x).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = to_torch(sample_from_tensor(features_source))\n",
    "Y = to_torch(sample_from_tensor(features_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_cost(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сбрасываем веса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_reset(T); weight_reset(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Параметр W из алгоритма неполного транспорта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "W=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм неполного оптимального транспорта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XNOT algo\n",
    "\n",
    "for step in tqdm (range(MAX_ITERS)):\n",
    "\n",
    "    T.train(True); f.eval()\n",
    "    for t_iter in range(T_ITERS):\n",
    "        X = to_torch(sample_from_tensor(features_source)) \n",
    "        T_loss = COST(X, T(X)).mean() - f(T(X)).mean()\n",
    "        T_opt.zero_grad(); T_loss.backward(); T_opt.step()\n",
    "\n",
    "    #f optimization\n",
    "    T.eval(); f.train(True)\n",
    "    X = to_torch(sample_from_tensor(features_source))\n",
    "    #Y = to_torch(sample_from_tensor(features_target))\n",
    "    Y_G = xnotvc.match(X, features_target)\n",
    "    Y = Y_G.clone()\n",
    "    f_loss = f(T(X)).mean() - (W * f(Y)).mean()\n",
    "    f_opt.zero_grad(); f_loss.backward(); f_opt.step()\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(\"Step\", step)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(T.state_dict(), \"checkpoints/xnot_vc_2300_3570_30s_20k_W_20.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Транспорт в новый домен"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция, которая заменяет каждый исходный вектор фичей на преобразованный"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xnot_transform(source_tensor):\n",
    "    with torch.no_grad():\n",
    "        result = torch.empty(source_tensor.shape[0],1024) #Initialize empty tensor with the same dimensions as an input feature vector\n",
    "        for i in range(source_tensor.shape[0]):\n",
    "            result[i] = T(source_tensor[i])\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загружаем звуковую дорожку с женским голосом, которой не было в тренировочной выборке и формируем массив из векторов фичей с помощью WavLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_path = \"experiments/2300/valid_long.flac\" #Путь до файла с женским голосом для валидации\n",
    "features_valid = xnotvc.get_features(validation_path) #Формируем массив из векторов фичей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features_valid.shape) #Исходный массив: 646 векторов размерностью 1024\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Формируем преобразованный массив"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "feautures_transformed = xnot_transform(features_valid) #Преобразованный массив"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feautures_transformed.shape) #Размерность преобразованного массива должна совпасть с размерностью исходного = 646х1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feautures_transformed[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вокодинг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применяем метод для вокодинга фичей с помощью HiFi GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_wav = xnotvc.vocode(feautures_transformed[None].to(DEVICE)).cpu().squeeze() #Вокодинг преобразованного с помощью XNOT вектора фичей\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out_wav.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраняем файл с аудиодорожкой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchaudio.save('otknn female_30.flac', out_wav[None], 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "not",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
