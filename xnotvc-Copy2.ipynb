{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-10T21:51:16.411730Z",
     "iopub.status.busy": "2024-08-10T21:51:16.411458Z",
     "iopub.status.idle": "2024-08-10T21:51:17.767835Z",
     "shell.execute_reply": "2024-08-10T21:51:17.766682Z",
     "shell.execute_reply.started": "2024-08-10T21:51:16.411706Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import logging\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "from wavlm.WavLM import WavLM, WavLMConfig\n",
    "from hifigan.models import Generator as HiFiGAN\n",
    "from hifigan.utils import AttrDict\n",
    "from matcher import ExNOTVC\n",
    "import torchaudio\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-10T21:51:17.769609Z",
     "iopub.status.busy": "2024-08-10T21:51:17.769082Z",
     "iopub.status.idle": "2024-08-10T21:51:17.784490Z",
     "shell.execute_reply": "2024-08-10T21:51:17.783877Z",
     "shell.execute_reply.started": "2024-08-10T21:51:17.769588Z"
    }
   },
   "outputs": [],
   "source": [
    "def xnot_vc(pretrained=True, progress=True, prematched=False, device='cuda') -> ExNOTVC:\n",
    "    \"\"\" Load kNN-VC (WavLM encoder and HiFiGAN decoder). Optionally use vocoder trained on `prematched` data. \"\"\"\n",
    "    hifigan, hifigan_cfg = hifigan_wavlm(pretrained, progress, prematched, device)\n",
    "    wavlm = wavlm_large(pretrained, progress, device)\n",
    "    xnotvc = ExNOTVC(wavlm, hifigan, hifigan_cfg, device)\n",
    "    return xnotvc\n",
    "\n",
    "\n",
    "def hifigan_wavlm(pretrained=True, progress=True, prematched=False, device='cuda') -> HiFiGAN:\n",
    "    \"\"\" Load pretrained hifigan trained to vocode wavlm features. Optionally use weights trained on `prematched` data. \"\"\"\n",
    "    #cp = Path(__file__).parent.absolute()\n",
    "\n",
    "    with open('hifigan/config_v1_wavlm.json') as f:\n",
    "        data = f.read()\n",
    "    json_config = json.loads(data)\n",
    "    h = AttrDict(json_config)\n",
    "    device = torch.device(device)\n",
    "\n",
    "    generator = HiFiGAN(h).to(device)\n",
    "    \n",
    "    if pretrained:\n",
    "        if prematched:\n",
    "            url = \"https://github.com/bshall/knn-vc/releases/download/v0.1/prematch_g_02500000.pt\"\n",
    "        else:\n",
    "            print(\"Загружаем непреметченный\")\n",
    "            url = \"https://github.com/bshall/knn-vc/releases/download/v0.1/g_02500000.pt\"\n",
    "        state_dict_g = torch.hub.load_state_dict_from_url(\n",
    "            url,\n",
    "            map_location=device,\n",
    "            progress=progress\n",
    "        )\n",
    "        generator.load_state_dict(state_dict_g['generator'])\n",
    "    generator.eval()\n",
    "    generator.remove_weight_norm()\n",
    "    print(f\"[HiFiGAN] Generator loaded with {sum([p.numel() for p in generator.parameters()]):,d} parameters.\")\n",
    "    return generator, h\n",
    "\n",
    "\n",
    "def wavlm_large(pretrained=True, progress=True, device='cuda') -> WavLM:\n",
    "    \"\"\"Load the WavLM large checkpoint from the original paper. See https://github.com/microsoft/unilm/tree/master/wavlm for details. \"\"\"\n",
    "    if torch.cuda.is_available() == False:\n",
    "        if str(device) != 'cpu':\n",
    "            logging.warning(f\"Overriding device {device} to cpu since no GPU is available.\")\n",
    "            device = 'cpu'\n",
    "    checkpoint = torch.hub.load_state_dict_from_url(\n",
    "        \"https://github.com/bshall/knn-vc/releases/download/v0.1/WavLM-Large.pt\", \n",
    "        map_location=device, \n",
    "        progress=progress\n",
    "    )\n",
    "    \n",
    "    cfg = WavLMConfig(checkpoint['cfg'])\n",
    "    device = torch.device(device)\n",
    "    model = WavLM(cfg)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    print(f\"WavLM-Large loaded with {sum([p.numel() for p in model.parameters()]):,d} parameters.\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-10T21:51:17.785383Z",
     "iopub.status.busy": "2024-08-10T21:51:17.785129Z",
     "iopub.status.idle": "2024-08-10T21:54:30.080205Z",
     "shell.execute_reply": "2024-08-10T21:54:30.079300Z",
     "shell.execute_reply.started": "2024-08-10T21:51:17.785366Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wlad/projects/smiles/xnot-vc/.venv/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загружаем непреметченный\n",
      "Removing weight norm...\n",
      "[HiFiGAN] Generator loaded with 16,523,393 parameters.\n",
      "WavLM-Large loaded with 315,453,120 parameters.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "xnotvc = xnot_vc() #загружаем веса для WavLM, HiFi Gan, убираем флажок преметчинга \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пути к аудиодорожкам с женским голосом и мужским"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Domain source - домен с исходным голосом, Domain target - домен с целевым голосом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-10T21:54:30.081844Z",
     "iopub.status.busy": "2024-08-10T21:54:30.081548Z",
     "iopub.status.idle": "2024-08-10T21:54:30.135581Z",
     "shell.execute_reply": "2024-08-10T21:54:30.134687Z",
     "shell.execute_reply.started": "2024-08-10T21:54:30.081825Z"
    }
   },
   "outputs": [],
   "source": [
    "domain_target_path = \"experiments/2300/to_female/female_4992.flac\"\n",
    "domain_source_path = \"experiments/2300/male_2300.flac\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кодируем аудиодорожки с помощью WavLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-10T21:54:30.136464Z",
     "iopub.status.busy": "2024-08-10T21:54:30.136206Z",
     "iopub.status.idle": "2024-08-10T21:54:32.210191Z",
     "shell.execute_reply": "2024-08-10T21:54:32.209495Z",
     "shell.execute_reply.started": "2024-08-10T21:54:30.136445Z"
    }
   },
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 4.21 GiB. GPU 0 has a total capacity of 15.68 GiB of which 2.77 GiB is free. Process 75184 has 408.00 MiB memory in use. Process 107714 has 3.55 GiB memory in use. Including non-PyTorch memory, this process has 6.89 GiB memory in use. Of the allocated memory 5.60 GiB is allocated by PyTorch, and 1.02 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m features_source \u001b[38;5;241m=\u001b[39m \u001b[43mxnotvc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdomain_source_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m features_target \u001b[38;5;241m=\u001b[39m xnotvc\u001b[38;5;241m.\u001b[39mget_features(domain_target_path)\n",
      "File \u001b[0;32m~/projects/smiles/xnot-vc/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/smiles/backup/xnot-vc/matcher.py:108\u001b[0m, in \u001b[0;36mExNOTVC.get_features\u001b[0;34m(self, path, weights, vad_trigger_level)\u001b[0m\n\u001b[1;32m    105\u001b[0m wav_input_16khz \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mallclose(weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweighting):\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m# use fastpath\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwavlm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwav_input_16khz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSPEAKER_INFORMATION_LAYER\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mret_layer_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    109\u001b[0m     features \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;66;03m# use slower weighted\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/smiles/backup/xnot-vc/wavlm/WavLM.py:364\u001b[0m, in \u001b[0;36mWavLM.extract_features\u001b[0;34m(self, source, padding_mask, mask, ret_conv, output_layer, ret_layer_results)\u001b[0m\n\u001b[1;32m    357\u001b[0m     x \u001b[38;5;241m=\u001b[39m features\n\u001b[1;32m    359\u001b[0m \u001b[38;5;66;03m# feature: (B, T, D), float\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;66;03m# target: (B, T), long\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;66;03m# x: (B, T, D), float\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;66;03m# padding_mask: (B, T), bool\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;66;03m# mask_indices: (B, T), bool\u001b[39;00m\n\u001b[0;32m--> 364\u001b[0m x, layer_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput_layer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput_layer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m    368\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    370\u001b[0m res \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m: x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpadding_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: padding_mask, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m: features, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayer_results\u001b[39m\u001b[38;5;124m\"\u001b[39m: layer_results}\n\u001b[1;32m    372\u001b[0m feature \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m ret_conv \u001b[38;5;28;01melse\u001b[39;00m res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/projects/smiles/xnot-vc/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/smiles/xnot-vc/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/smiles/backup/xnot-vc/wavlm/WavLM.py:565\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, x, padding_mask, streaming_mask, layer)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, padding_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, streaming_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, layer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 565\u001b[0m     x, layer_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstreaming_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm_first \u001b[38;5;129;01mand\u001b[39;00m layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    568\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(x)\n",
      "File \u001b[0;32m~/projects/smiles/backup/xnot-vc/wavlm/WavLM.py:598\u001b[0m, in \u001b[0;36mTransformerEncoder.extract_features\u001b[0;34m(self, x, padding_mask, streaming_mask, tgt_layer)\u001b[0m\n\u001b[1;32m    596\u001b[0m dropout_probability \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandom()\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mor\u001b[39;00m (dropout_probability \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayerdrop):\n\u001b[0;32m--> 598\u001b[0m     x, z, pos_bias \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_attn_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mself_attn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreaming_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_bias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tgt_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    601\u001b[0m     layer_results\u001b[38;5;241m.\u001b[39mappend((x, z))\n",
      "File \u001b[0;32m~/projects/smiles/xnot-vc/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/smiles/xnot-vc/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/smiles/backup/xnot-vc/wavlm/WavLM.py:693\u001b[0m, in \u001b[0;36mTransformerSentenceEncoderLayer.forward\u001b[0;34m(self, x, self_attn_mask, self_attn_padding_mask, need_weights, pos_bias)\u001b[0m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm_first:\n\u001b[1;32m    692\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn_layer_norm(x)\n\u001b[0;32m--> 693\u001b[0m     x, attn, pos_bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_bias\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(x)\n\u001b[1;32m    703\u001b[0m     x \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m x\n",
      "File \u001b[0;32m~/projects/smiles/xnot-vc/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/smiles/xnot-vc/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/smiles/backup/xnot-vc/wavlm/modules.py:506\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, incremental_state, need_weights, static_kv, attn_mask, before_softmax, need_head_weights, position_bias)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_relative_attention_bias \u001b[38;5;129;01mand\u001b[39;00m position_bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    505\u001b[0m     position_bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_bias(tgt_len, src_len)\n\u001b[0;32m--> 506\u001b[0m     position_bias \u001b[38;5;241m=\u001b[39m \u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbsz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(bsz \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, tgt_len, src_len)\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    509\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m is_tpu  \u001b[38;5;66;03m# don't use PyTorch version on TPUs\u001b[39;00m\n\u001b[1;32m    510\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m incremental_state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_head_dim \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim\n\u001b[1;32m    516\u001b[0m ):\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 4.21 GiB. GPU 0 has a total capacity of 15.68 GiB of which 2.77 GiB is free. Process 75184 has 408.00 MiB memory in use. Process 107714 has 3.55 GiB memory in use. Including non-PyTorch memory, this process has 6.89 GiB memory in use. Of the allocated memory 5.60 GiB is allocated by PyTorch, and 1.02 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "features_source = xnotvc.get_features(domain_source_path)\n",
    "features_target = xnotvc.get_features(domain_target_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **KNNVC TEST** (skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-10T21:54:32.211265Z",
     "iopub.status.busy": "2024-08-10T21:54:32.211009Z",
     "iopub.status.idle": "2024-08-10T21:54:32.267609Z",
     "shell.execute_reply": "2024-08-10T21:54:32.267073Z",
     "shell.execute_reply.started": "2024-08-10T21:54:32.211248Z"
    }
   },
   "outputs": [],
   "source": [
    "validation_path = \"experiments/2300/valid_long.flac\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-10T21:54:32.268442Z",
     "iopub.status.busy": "2024-08-10T21:54:32.268206Z",
     "iopub.status.idle": "2024-08-10T21:54:32.297076Z",
     "shell.execute_reply": "2024-08-10T21:54:32.296563Z",
     "shell.execute_reply.started": "2024-08-10T21:54:32.268427Z"
    }
   },
   "outputs": [],
   "source": [
    "ref_wav_paths = [domain_target_path] #target\n",
    "src_wav_path = validation_path  #validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-10T21:54:32.298122Z",
     "iopub.status.busy": "2024-08-10T21:54:32.297691Z",
     "iopub.status.idle": "2024-08-10T21:54:32.652139Z",
     "shell.execute_reply": "2024-08-10T21:54:32.651494Z",
     "shell.execute_reply.started": "2024-08-10T21:54:32.298105Z"
    }
   },
   "outputs": [],
   "source": [
    "query_seq = xnotvc.get_features(src_wav_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-10T21:54:32.653154Z",
     "iopub.status.busy": "2024-08-10T21:54:32.652907Z",
     "iopub.status.idle": "2024-08-10T21:54:34.080511Z",
     "shell.execute_reply": "2024-08-10T21:54:34.079709Z",
     "shell.execute_reply.started": "2024-08-10T21:54:32.653137Z"
    }
   },
   "outputs": [],
   "source": [
    "matching_set = xnotvc.get_matching_set(ref_wav_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-10T21:54:34.083011Z",
     "iopub.status.busy": "2024-08-10T21:54:34.082716Z",
     "iopub.status.idle": "2024-08-10T21:54:34.298361Z",
     "shell.execute_reply": "2024-08-10T21:54:34.297646Z",
     "shell.execute_reply.started": "2024-08-10T21:54:34.082993Z"
    }
   },
   "outputs": [],
   "source": [
    "out_wav = xnotvc.match(query_seq, matching_set, topk=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-10T21:54:34.299732Z",
     "iopub.status.busy": "2024-08-10T21:54:34.299469Z",
     "iopub.status.idle": "2024-08-10T21:54:34.393806Z",
     "shell.execute_reply": "2024-08-10T21:54:34.393213Z",
     "shell.execute_reply.started": "2024-08-10T21:54:34.299714Z"
    }
   },
   "outputs": [],
   "source": [
    "torchaudio.save('experiments/2300/to_female/results/knnvc/knnvc_2300_4992.flac', out_wav[None], 16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **END OF TEST**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WavLM каждые ~20-30 мс звуковой дорожки представляет в виде вектора фичей размерностью 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-10T21:54:34.394892Z",
     "iopub.status.busy": "2024-08-10T21:54:34.394642Z",
     "iopub.status.idle": "2024-08-10T21:54:34.403539Z",
     "shell.execute_reply": "2024-08-10T21:54:34.402994Z",
     "shell.execute_reply.started": "2024-08-10T21:54:34.394875Z"
    }
   },
   "outputs": [],
   "source": [
    "print(features_source.shape) #[длина дорожки делить на ~20-30мс , 1024] = массив из 7971 векторов размерностью 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-10T21:54:34.404395Z",
     "iopub.status.busy": "2024-08-10T21:54:34.404156Z",
     "iopub.status.idle": "2024-08-10T21:54:34.413609Z",
     "shell.execute_reply": "2024-08-10T21:54:34.413033Z",
     "shell.execute_reply.started": "2024-08-10T21:54:34.404380Z"
    }
   },
   "outputs": [],
   "source": [
    "print(features_target.shape) #массив из 5473 векторов размерностью 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampler from voice distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Случайным образом выбираем батч векторов фичей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-10T21:54:34.414493Z",
     "iopub.status.busy": "2024-08-10T21:54:34.414253Z",
     "iopub.status.idle": "2024-08-10T21:54:34.424618Z",
     "shell.execute_reply": "2024-08-10T21:54:34.424191Z",
     "shell.execute_reply.started": "2024-08-10T21:54:34.414477Z"
    }
   },
   "outputs": [],
   "source": [
    "print(features_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-10T21:54:34.425450Z",
     "iopub.status.busy": "2024-08-10T21:54:34.425142Z",
     "iopub.status.idle": "2024-08-10T21:54:34.435018Z",
     "shell.execute_reply": "2024-08-10T21:54:34.434577Z",
     "shell.execute_reply.started": "2024-08-10T21:54:34.425435Z"
    }
   },
   "outputs": [],
   "source": [
    "def sample_from_tensor(t, batch_size = 32):\n",
    "    tensor = t.detach().cpu().numpy()\n",
    "    batch = np.empty((batch_size, 1024))\n",
    "    for i in range(batch_size):\n",
    "        indice = random.randrange(0, tensor.shape[0])\n",
    "        sample_tensor = tensor[indice]\n",
    "        batch[i]=sample_tensor\n",
    "    return batch.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-10T21:54:34.436055Z",
     "iopub.status.busy": "2024-08-10T21:54:34.435544Z",
     "iopub.status.idle": "2024-08-10T21:54:34.450248Z",
     "shell.execute_reply": "2024-08-10T21:54:34.449597Z",
     "shell.execute_reply.started": "2024-08-10T21:54:34.436038Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_from_tensor(features_source).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-10T21:54:34.451074Z",
     "iopub.status.busy": "2024-08-10T21:54:34.450813Z",
     "iopub.status.idle": "2024-08-10T21:54:34.465866Z",
     "shell.execute_reply": "2024-08-10T21:54:34.465439Z",
     "shell.execute_reply.started": "2024-08-10T21:54:34.451058Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_from_tensor(features_target).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция стоимости"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-10T21:54:34.466868Z",
     "iopub.status.busy": "2024-08-10T21:54:34.466415Z",
     "iopub.status.idle": "2024-08-10T21:54:34.478922Z",
     "shell.execute_reply": "2024-08-10T21:54:34.478501Z",
     "shell.execute_reply.started": "2024-08-10T21:54:34.466849Z"
    }
   },
   "outputs": [],
   "source": [
    "def sq_cost(X,Y):\n",
    "    return (X-Y).square().flatten(start_dim=1).mean(dim=1)\n",
    "\n",
    "COST = sq_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define NNs for transport map ***T*** : R^1024 -> R^1024 (generator) and potential ***f*** : R^1024 -> R (discriminator).\n",
    "Medium size multilayer perceptrons with ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-10T21:54:34.480069Z",
     "iopub.status.busy": "2024-08-10T21:54:34.479493Z",
     "iopub.status.idle": "2024-08-10T21:54:34.578668Z",
     "shell.execute_reply": "2024-08-10T21:54:34.578102Z",
     "shell.execute_reply.started": "2024-08-10T21:54:34.480046Z"
    }
   },
   "outputs": [],
   "source": [
    "class NegAbs(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NegAbs, self).__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return -torch.abs(input)\n",
    "\n",
    "T = nn.Sequential(\n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(1024, 1024),\n",
    ").to(DEVICE)\n",
    "\n",
    "f = nn.Sequential(\n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(1024, 1),\n",
    "    NegAbs(),\n",
    ").to(DEVICE)\n",
    "\n",
    "def weight_reset(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        m.reset_parameters()\n",
    "\n",
    "\n",
    "print('T params:', np.sum([np.prod(p.shape) for p in T.parameters()]))\n",
    "print('f params:', np.sum([np.prod(p.shape) for p in f.parameters()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оптимизаторы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-10T21:54:34.579655Z",
     "iopub.status.busy": "2024-08-10T21:54:34.579364Z",
     "iopub.status.idle": "2024-08-10T21:54:34.589654Z",
     "shell.execute_reply": "2024-08-10T21:54:34.589033Z",
     "shell.execute_reply.started": "2024-08-10T21:54:34.579638Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "T_opt = torch.optim.Adam(T.parameters(), lr=1e-4, weight_decay=1e-10)\n",
    "f_opt = torch.optim.Adam(f.parameters(), lr=1e-4, weight_decay=1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-10T21:54:34.590394Z",
     "iopub.status.busy": "2024-08-10T21:54:34.590173Z",
     "iopub.status.idle": "2024-08-10T21:54:34.597707Z",
     "shell.execute_reply": "2024-08-10T21:54:34.597276Z",
     "shell.execute_reply.started": "2024-08-10T21:54:34.590379Z"
    }
   },
   "outputs": [],
   "source": [
    "#Parameters\n",
    "T_ITERS = 10 # T updates per 1 f update\n",
    "MAX_ITERS = 20001\n",
    "\n",
    "to_torch = lambda x: torch.Tensor(x).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-10T21:54:34.598531Z",
     "iopub.status.busy": "2024-08-10T21:54:34.598222Z",
     "iopub.status.idle": "2024-08-10T21:54:34.614232Z",
     "shell.execute_reply": "2024-08-10T21:54:34.613795Z",
     "shell.execute_reply.started": "2024-08-10T21:54:34.598516Z"
    }
   },
   "outputs": [],
   "source": [
    "X = to_torch(sample_from_tensor(features_source))\n",
    "Y = to_torch(sample_from_tensor(features_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-10T21:54:34.615005Z",
     "iopub.status.busy": "2024-08-10T21:54:34.614753Z",
     "iopub.status.idle": "2024-08-10T21:54:34.741443Z",
     "shell.execute_reply": "2024-08-10T21:54:34.740898Z",
     "shell.execute_reply.started": "2024-08-10T21:54:34.614988Z"
    }
   },
   "outputs": [],
   "source": [
    "sq_cost(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сбрасываем веса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-10T21:54:34.742646Z",
     "iopub.status.busy": "2024-08-10T21:54:34.742097Z",
     "iopub.status.idle": "2024-08-10T21:54:34.752165Z",
     "shell.execute_reply": "2024-08-10T21:54:34.751729Z",
     "shell.execute_reply.started": "2024-08-10T21:54:34.742626Z"
    }
   },
   "outputs": [],
   "source": [
    "weight_reset(T); weight_reset(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Параметр W из алгоритма неполного транспорта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-10T21:54:34.752938Z",
     "iopub.status.busy": "2024-08-10T21:54:34.752700Z",
     "iopub.status.idle": "2024-08-10T21:54:34.761181Z",
     "shell.execute_reply": "2024-08-10T21:54:34.760757Z",
     "shell.execute_reply.started": "2024-08-10T21:54:34.752922Z"
    }
   },
   "outputs": [],
   "source": [
    "W=16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм неполного оптимального транспорта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-10T21:54:34.762004Z",
     "iopub.status.busy": "2024-08-10T21:54:34.761710Z",
     "iopub.status.idle": "2024-08-10T22:25:02.299030Z",
     "shell.execute_reply": "2024-08-10T22:25:02.298136Z",
     "shell.execute_reply.started": "2024-08-10T21:54:34.761988Z"
    }
   },
   "outputs": [],
   "source": [
    "#XNOT algo\n",
    "\n",
    "for step in tqdm (range(MAX_ITERS)):\n",
    "    T_loss_ar = []\n",
    "    f_loss_ar = []\n",
    "    #T optimization\n",
    "    T.train(True); f.eval()\n",
    "    for t_iter in range(T_ITERS):\n",
    "        X = to_torch(sample_from_tensor(features_source)) \n",
    "        T_loss = COST(X, T(X)).mean() - f(T(X)).mean()\n",
    "        with torch.no_grad():\n",
    "            T_loss_ar.append(T_loss.cpu().detach().numpy().mean())\n",
    "        T_opt.zero_grad(); T_loss.backward(); T_opt.step()\n",
    "\n",
    "    #f optimization\n",
    "    T.eval(); f.train(True)\n",
    "    X, Y = to_torch(sample_from_tensor(features_source)), to_torch(sample_from_tensor(features_target))\n",
    "    f_loss = f(T(X)).mean() - (W * f(Y)).mean()\n",
    "    with torch.no_grad():\n",
    "            f_loss_ar.append(f_loss.cpu().detach().numpy())\n",
    "    f_opt.zero_grad(); f_loss.backward(); f_opt.step()\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(\"Step\", step)\n",
    "        print(\"T_loss\", T_loss_ar)\n",
    "        print(\"f_loss\", f_loss_ar)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-10T22:25:02.299972Z",
     "iopub.status.busy": "2024-08-10T22:25:02.299719Z",
     "iopub.status.idle": "2024-08-10T22:25:02.569493Z",
     "shell.execute_reply": "2024-08-10T22:25:02.568449Z",
     "shell.execute_reply.started": "2024-08-10T22:25:02.299954Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(T.state_dict(), \"xnot_vc_2300_4992_W_16.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Транспорт в новый домен"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция, которая заменяет каждый исходный вектор фичей на преобразованный"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-10T22:25:02.576697Z",
     "iopub.status.busy": "2024-08-10T22:25:02.576408Z",
     "iopub.status.idle": "2024-08-10T22:25:02.622162Z",
     "shell.execute_reply": "2024-08-10T22:25:02.621511Z",
     "shell.execute_reply.started": "2024-08-10T22:25:02.576679Z"
    }
   },
   "outputs": [],
   "source": [
    "def xnot_transform(source_tensor):\n",
    "    with torch.no_grad():\n",
    "        result = torch.empty(source_tensor.shape[0],1024) #Initialize empty tensor with the same dimensions as an input feature vector\n",
    "        for i in range(source_tensor.shape[0]):\n",
    "            result[i] = T(source_tensor[i])\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загружаем звуковую дорожку с женским голосом, которой не было в тренировочной выборке и формируем массив из векторов фичей с помощью WavLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-10T22:25:02.623336Z",
     "iopub.status.busy": "2024-08-10T22:25:02.622886Z",
     "iopub.status.idle": "2024-08-10T22:25:02.679902Z",
     "shell.execute_reply": "2024-08-10T22:25:02.679136Z",
     "shell.execute_reply.started": "2024-08-10T22:25:02.623319Z"
    }
   },
   "outputs": [],
   "source": [
    "validation_path = \"experiments/2300/valid_long.flac\" #Путь до файла с женским голосом для валидации\n",
    "features_valid = xnotvc.get_features(validation_path) #Формируем массив из векторов фичей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-10T22:25:02.681232Z",
     "iopub.status.busy": "2024-08-10T22:25:02.680967Z",
     "iopub.status.idle": "2024-08-10T22:25:02.733554Z",
     "shell.execute_reply": "2024-08-10T22:25:02.732891Z",
     "shell.execute_reply.started": "2024-08-10T22:25:02.681215Z"
    }
   },
   "outputs": [],
   "source": [
    "print(features_valid.shape) #Исходный массив: 646 векторов размерностью 1024\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Формируем преобразованный массив"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-10T22:25:02.734613Z",
     "iopub.status.busy": "2024-08-10T22:25:02.734216Z",
     "iopub.status.idle": "2024-08-10T22:25:03.068363Z",
     "shell.execute_reply": "2024-08-10T22:25:03.067616Z",
     "shell.execute_reply.started": "2024-08-10T22:25:02.734595Z"
    }
   },
   "outputs": [],
   "source": [
    "feautures_transformed = xnot_transform(features_valid) #Преобразованный массив"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-10T22:25:03.069457Z",
     "iopub.status.busy": "2024-08-10T22:25:03.069130Z",
     "iopub.status.idle": "2024-08-10T22:25:03.084200Z",
     "shell.execute_reply": "2024-08-10T22:25:03.083525Z",
     "shell.execute_reply.started": "2024-08-10T22:25:03.069438Z"
    }
   },
   "outputs": [],
   "source": [
    "print(feautures_transformed.shape) #Размерность преобразованного массива должна совпасть с размерностью исходного = 646х1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-10T22:25:03.085228Z",
     "iopub.status.busy": "2024-08-10T22:25:03.084950Z",
     "iopub.status.idle": "2024-08-10T22:25:03.099852Z",
     "shell.execute_reply": "2024-08-10T22:25:03.098995Z",
     "shell.execute_reply.started": "2024-08-10T22:25:03.085212Z"
    }
   },
   "outputs": [],
   "source": [
    "print(feautures_transformed[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вокодинг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применяем метод для вокодинга фичей с помощью HiFi GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-10T22:25:03.101104Z",
     "iopub.status.busy": "2024-08-10T22:25:03.100544Z",
     "iopub.status.idle": "2024-08-10T22:25:03.141104Z",
     "shell.execute_reply": "2024-08-10T22:25:03.140465Z",
     "shell.execute_reply.started": "2024-08-10T22:25:03.101085Z"
    }
   },
   "outputs": [],
   "source": [
    "out_wav = xnotvc.vocode(feautures_transformed[None].to(DEVICE)).cpu().squeeze() #Вокодинг преобразованного с помощью XNOT вектора фичей\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-10T22:25:03.142071Z",
     "iopub.status.busy": "2024-08-10T22:25:03.141781Z",
     "iopub.status.idle": "2024-08-10T22:25:03.152228Z",
     "shell.execute_reply": "2024-08-10T22:25:03.151671Z",
     "shell.execute_reply.started": "2024-08-10T22:25:03.142054Z"
    }
   },
   "outputs": [],
   "source": [
    "print(out_wav.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраняем файл с аудиодорожкой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-10T22:25:03.153187Z",
     "iopub.status.busy": "2024-08-10T22:25:03.152867Z",
     "iopub.status.idle": "2024-08-10T22:25:03.213953Z",
     "shell.execute_reply": "2024-08-10T22:25:03.213284Z",
     "shell.execute_reply.started": "2024-08-10T22:25:03.153150Z"
    }
   },
   "outputs": [],
   "source": [
    "torchaudio.save('experiments/2300/to_female/results/xnot/xnotvc_2300_4992.flac', out_wav[None], 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
