{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbf0a1d-b92e-4365-854d-3e89bc195d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install speechbrain whisper jiwer pandas -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4a94d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install git+https://github.com/openai/whisper.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ecd8719-a028-48c9-8cc2-b35cd5deac7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import whisper\n",
    "import jiwer\n",
    "#import intel_extension_for_pytorch as ipex\n",
    "import torch\n",
    "import torchaudio\n",
    "import pathlib\n",
    "import collections\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "asr = whisper.load_model(\"base\", device=\"cpu\",)\n",
    "\n",
    "def calculate_wer_cer(reference_text, audio_file_path):\n",
    "    \n",
    "    # Transcribe the audio file\n",
    "    result = asr.transcribe(audio_file_path, )\n",
    "    transcribed_text = result['text']\n",
    "    \n",
    "    # Calculate WER\n",
    "    wer = jiwer.wer(reference_text, transcribed_text)\n",
    "    \n",
    "    # Calculate CER\n",
    "    cer = jiwer.cer(reference_text, transcribed_text)\n",
    "    \n",
    "    return wer, cer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07d9a8e5-2f67-432f-9f12-202187850522",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = pathlib.Path(\"experiments/2300/\")\n",
    "\n",
    "with open(path.joinpath(\"valid ground truth.txt\"), \"r\") as f:\n",
    "    text = f.read().lower()\n",
    "\n",
    "reference_audio = path.joinpath(\"valid_long.flac\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4eb1a1-6e41-4861-8693-2a921a79207e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464e2c47-dc6d-42ec-b363-c59aaaee2806",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wer_ref, cer_ref = calculate_wer_cer(text, reference_audio.as_posix())\n",
    "wer_ref, cer_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296520a9-c773-41e6-8119-78aac71c4559",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "asr_knn_dict = collections.defaultdict(dict)\n",
    "\n",
    "for item in path.joinpath(\"to_male/results/linreg\").iterdir():\n",
    "    \n",
    "    wer, cer = calculate_wer_cer(text, item.as_posix())\n",
    "    \n",
    "    asr_knn_dict[item.stem + \"_male\"] = {\"wer\": wer, \"cer\": cer}\n",
    "    \n",
    "for item in path.joinpath(\"to_female/results/linreg\").iterdir():\n",
    "    \n",
    "    wer, cer = calculate_wer_cer(text, item.as_posix())\n",
    "    \n",
    "    asr_knn_dict[item.stem + \"_female\"] = {\"wer\": wer, \"cer\": cer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08df5bbd-84a3-42cd-9c4b-54a2685144f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "asr_xnot_dict = collections.defaultdict(dict)\n",
    "\n",
    "for item in path.joinpath(\"to_male/results/xnot-vc\").iterdir():\n",
    "    \n",
    "    wer, cer = calculate_wer_cer(text, item.as_posix())\n",
    "    \n",
    "    asr_xnot_dict[item.stem + \"_male\"] = {\"wer\": wer, \"cer\": cer}\n",
    "    \n",
    "for item in path.joinpath(\"to_female/results/xnot-vc\").iterdir():\n",
    "    \n",
    "    wer, cer = calculate_wer_cer(text, item.as_posix())\n",
    "    \n",
    "    asr_xnot_dict[item.stem + \"_female\"] = {\"wer\": wer, \"cer\": cer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6faf335",
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_linreg_dict = collections.defaultdict(dict)\n",
    "\n",
    "for item in path.joinpath(\"to_male/results/linreg\").iterdir():\n",
    "    \n",
    "    wer, cer = calculate_wer_cer(text, item.as_posix())\n",
    "    \n",
    "    asr_linreg_dict[item.stem + \"_male\"] = {\"wer\": wer, \"cer\": cer}\n",
    "    \n",
    "for item in path.joinpath(\"to_female/results/linreg\").iterdir():\n",
    "    \n",
    "    wer, cer = calculate_wer_cer(text, item.as_posix())\n",
    "    \n",
    "    asr_linreg_dict[item.stem + \"_female\"] = {\"wer\": wer, \"cer\": cer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "649b9793-6697-4b4e-98a7-17bbbb70144c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "linreg_df = pd.DataFrame.from_dict(asr_linreg_dict).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1119950b-12c3-4e63-a64d-c68c3b43fdeb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "linreg_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7f2317-beda-4cfc-b45d-1673b1338083",
   "metadata": {},
   "source": [
    "### EER calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f569a0ff-db5b-4a20-848e-75242b6d0293",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = pathlib.Path(\"experiments/2300/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "343653e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import speechbrain.inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a0159d3-8298-40c4-aa3f-f5aef646735d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from speechbrain.inference import EncoderClassifier\n",
    "speaker_enc = EncoderClassifier.from_hparams(\n",
    "  \"speechbrain/spkrec-xvect-voxceleb\"\n",
    ")\n",
    "\n",
    "speaker_enc.eval();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ff1b1c6-8767-47bc-b224-2dcddd3ec7f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_speaker_matrix(path, glob):\n",
    "\n",
    "    matrix = None\n",
    "\n",
    "    for p in path.joinpath(\"to_male\").rglob(glob):\n",
    "\n",
    "        signal, fs = torchaudio.load(p)\n",
    "        embeddings = speaker_enc.encode_batch(signal)\n",
    "\n",
    "        if matrix is None:\n",
    "            matrix = embeddings\n",
    "        else: \n",
    "            matrix = torch.cat((matrix, embeddings), dim=0)\n",
    "            \n",
    "    for p in path.joinpath(\"to_female\").rglob(glob):\n",
    "\n",
    "        signal, fs = torchaudio.load(p)\n",
    "        embeddings = speaker_enc.encode_batch(signal)\n",
    "        matrix = torch.cat((matrix, embeddings), dim=0)\n",
    "            \n",
    "    return matrix.squeeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b2a422b-eba2-449f-a32c-26b675a7b2e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'squeeze'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ref_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mget_speaker_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m*_30.flac\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 21\u001b[0m, in \u001b[0;36mget_speaker_matrix\u001b[0;34m(path, glob)\u001b[0m\n\u001b[1;32m     18\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m speaker_enc\u001b[38;5;241m.\u001b[39mencode_batch(signal)\n\u001b[1;32m     19\u001b[0m     matrix \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((matrix, embeddings), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmatrix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'squeeze'"
     ]
    }
   ],
   "source": [
    "ref_matrix = get_speaker_matrix(path, \"*_30.flac\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a15760d-a29e-4849-8eeb-cc765349d313",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "true_matrix = get_speaker_matrix(path, \"*_v2.flac\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d7c00a2c-a6f9-47b8-8044-8596b12350a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "knn_matrix = get_speaker_matrix(path, \"knnvc*.flac\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "21c9534b-5861-4567-981d-2927f699944f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xnot_matrix = get_speaker_matrix(path, \"xnotvc*.flac\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e28005-bb7d-4422-a9e0-da4543f27347",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.nn.functional.cosine_similarity(ref_matrix, true_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adf671f-c2cf-4493-8d30-7bd7fbba171f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.nn.functional.cosine_similarity(ref_matrix, knn_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e55daf4-30a9-4c9c-ac74-2065569bbeb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.nn.functional.cosine_similarity(ref_matrix, xnot_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e17a22-90bf-4d7f-837d-23427d987cbd",
   "metadata": {},
   "source": [
    "##### KNN-VC EER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7db46433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import speechbrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea69d9a-b2fc-469e-988b-1d72e9d8b732",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "speechbrain.utils.metric_stats.EER(\n",
    "    torch.nn.functional.cosine_similarity(ref_matrix, true_matrix),\n",
    "    torch.nn.functional.cosine_similarity(ref_matrix, knn_matrix)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130b7910-4ecd-4978-a6bf-645a5e75f0a8",
   "metadata": {},
   "source": [
    "##### XNOT-VC EER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdebf0f-f6e0-4cf0-b953-6cbf62f72d44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "speechbrain.utils.metric_stats.EER(\n",
    "    torch.nn.functional.cosine_similarity(ref_matrix, true_matrix),\n",
    "    torch.nn.functional.cosine_similarity(ref_matrix, xnot_matrix)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
